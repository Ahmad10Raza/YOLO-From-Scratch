{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3851f25b",
   "metadata": {},
   "source": [
    "### 🧠 YOLOv1 – You Only Look Once (Version 1) – Detailed Explanation\n",
    "\n",
    "\n",
    "YOLOv1, introduced by **Joseph Redmon et al. in 2015**, marked a major shift in object detection by framing it as a **single regression problem**, rather than a series of region proposals and classifications like R-CNNs.\n",
    "\n",
    "It offered **real-time object detection** with a unique approach: **one neural network processes the entire image in one forward pass**, hence the name **\"You Only Look Once.\"**\n",
    "\n",
    "\n",
    "\n",
    "### 📌 Key Innovations in YOLOv1\n",
    "\n",
    "1. **Unified Architecture:** Single CNN for bounding box and class prediction.\n",
    "2. **Grid-Based Prediction:** Image divided into an S×S grid (typically 7×7).\n",
    "3. **End-to-End Training:** Entire model trained using a single loss function.\n",
    "4. **Fast and Simple:** Real-time performance on standard hardware.\n",
    "\n",
    "\n",
    "\n",
    "### 🔧 Architecture Overview\n",
    "\n",
    "1. **Input Size:** 448×448 RGB image.\n",
    "2. **Convolutional Layers:** 24 convolutional layers (feature extraction).\n",
    "3. **Fully Connected Layers:** 2 FC layers (for predicting bounding boxes and class scores).\n",
    "4. **Output:** A tensor of size **S × S × (B × 5 + C)**, where:\n",
    "\n",
    "   * **S = 7** (grid size)\n",
    "   * **B = 2** (number of boxes per grid cell)\n",
    "   * **5** = (x, y, w, h, confidence)\n",
    "   * **C = number of classes**\n",
    "\n",
    "Thus, for Pascal VOC (20 classes):\n",
    "\n",
    "$$\n",
    "7 × 7 × (2 × 5 + 20) = 7 × 7 × 30 = 1470 \\text{ outputs}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 🧮 How YOLOv1 Works\n",
    "\n",
    "#### 1. **Grid Division**\n",
    "\n",
    "* The image is divided into a **7x7 grid**.\n",
    "* Each grid cell is responsible for detecting an object **only if the object’s center falls inside it**.\n",
    "\n",
    "#### 2. **Bounding Box Prediction**\n",
    "\n",
    "Each grid cell predicts:\n",
    "\n",
    "* **2 bounding boxes**:\n",
    "\n",
    "  * Each box has 5 predictions: `(x, y, w, h, confidence)`\n",
    "  * `(x, y)` are relative to the grid cell, and `w`, `h` are relative to the whole image.\n",
    "* **1 set of class probabilities**:\n",
    "\n",
    "  * Shared across both bounding boxes.\n",
    "  * 20 probabilities for 20 classes in Pascal VOC.\n",
    "\n",
    "#### 3. **Confidence Score**\n",
    "\n",
    "For each bounding box, the confidence score is:\n",
    "\n",
    "$$\n",
    "\\text{Confidence} = P(\\text{object}) \\times \\text{IoU}_{\\text{pred, truth}}\n",
    "$$\n",
    "\n",
    "This score indicates:\n",
    "\n",
    "* Whether an object is present.\n",
    "* How well the predicted box matches the ground truth box (via IoU).\n",
    "\n",
    "\n",
    "\n",
    "### 🧠 Loss Function (Recap from earlier)\n",
    "\n",
    "YOLOv1 uses a **custom loss function** that penalizes:\n",
    "\n",
    "* Localization error (bounding box coordinates).\n",
    "* Confidence score error (objectness).\n",
    "* Classification error (class probabilities).\n",
    "\n",
    "It includes weights for:\n",
    "\n",
    "* Boxes with and without objects (`λ_coord`, `λ_noobj`).\n",
    "* Uses **Mean Squared Error (MSE)** for all parts of the loss.\n",
    "\n",
    "\n",
    "\n",
    "### 📉 Limitations of YOLOv1\n",
    "\n",
    "| Issue                                       | Description                                                           |\n",
    "| ------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| 🧠 Struggles with small/overlapping objects | One object per grid cell – can’t detect multiple objects in one cell. |\n",
    "| 🧠 Poor localization for unusual shapes     | Bounding box prediction isn't as precise as region proposal methods.  |\n",
    "| 🧠 Uses MSE for classification              | Not optimal for multi-class probabilities.                            |\n",
    "| 🧠 Fixed number of boxes                    | Can’t handle variable number of objects per image.                    |\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Strengths of YOLOv1\n",
    "\n",
    "| Feature                | Benefit                                                              |\n",
    "| ---------------------- | -------------------------------------------------------------------- |\n",
    "| ⚡ Real-time Detection  | \\~45 FPS on standard GPU (fastest at the time).                      |\n",
    "| 🔄 Unified Pipeline    | Single model, end-to-end trainable.                                  |\n",
    "| 🔍 Global Context      | Looks at the whole image at once, unlike R-CNN which looks at parts. |\n",
    "| 🧩 Simple Architecture | Easy to train and deploy.                                            |\n",
    "\n",
    "\n",
    "\n",
    "### 📚 Paper Reference:\n",
    "\n",
    "**Title**: You Only Look Once: Unified, Real-Time Object Detection\n",
    "**Authors**: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\n",
    "**Published**: 2015 (CVPR)\n",
    "[Link to Paper](https://arxiv.org/abs/1506.02640)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048682d",
   "metadata": {},
   "source": [
    "## Implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519a982",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4aa405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "import skimage\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "seed = 123\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "torch.manual_seed(seed)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bc25f",
   "metadata": {},
   "source": [
    "## Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa6ecef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_config = [\n",
    "    # (kernel_size, filters, stride, padding)\n",
    "    (7, 64, 2, 3),          # Conv layer: 7x7 kernel, 64 filters, stride=2, padding=3 -> reduces spatial dim from 448x448 to 224x224\n",
    "    \"M\",                   # MaxPooling: 2x2 with stride 2 -> reduces spatial dim to 112x112\n",
    "\n",
    "\n",
    "\n",
    "    (3, 192, 1, 1),        # Conv layer: 3x3 kernel, 192 filters, stride=1, padding=1 -> keeps spatial dim 112x112\n",
    "    \"M\",                   # MaxPooling -> reduces spatial dim to 56x56\n",
    "\n",
    "\n",
    "    (1, 128, 1, 0),        # Conv: 1x1 kernel, 128 filters, stride=1, no padding -> used for dimensionality reduction\n",
    "    (3, 256, 1, 1),        # Conv: 3x3 kernel, 256 filters, stride=1, padding=1\n",
    "    (1, 256, 1, 0),        # Conv: 1x1, 256 filters\n",
    "    (3, 512, 1, 1),        # Conv: 3x3, 512 filters\n",
    "    \"M\",                   # MaxPooling -> spatial dim becomes 28x28\n",
    "\n",
    "\n",
    "\n",
    "    # This block is repeated 4 times:\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    # → Adds: 1x1 conv (256 filters) followed by 3x3 conv (512 filters), repeated 4 times\n",
    "    \n",
    "\n",
    "\n",
    "    (1, 512, 1, 0),        # Conv: 1x1, 512 filters\n",
    "    (3, 1024, 1, 1),       # Conv: 3x3, 1024 filters\n",
    "    \"M\",                   # MaxPooling -> spatial dim becomes 14x14\n",
    "\n",
    "\n",
    "    # This block is repeated 2 times:\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    # → Adds: 1x1 conv (512 filters) followed by 3x3 conv (1024 filters), repeated 2 times\n",
    "\n",
    "    (3, 1024, 1, 1),       # Conv: 3x3, 1024 filters\n",
    "    (3, 1024, 2, 1),       # Conv: 3x3, 1024 filters, stride=2 → spatial dim becomes 7x7\n",
    "    (3, 1024, 1, 1),       # Conv: 3x3, 1024 filters\n",
    "    (3, 1024, 1, 1),       # Conv: 3x3, 1024 filters\n",
    "\n",
    "    # Fully connected (FC) layers will be added later separately\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495ef98",
   "metadata": {},
   "source": [
    "#### 🔷 1. CNNBlock Class – Basic Convolution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f308aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707dec27",
   "metadata": {},
   "source": [
    "✅ Explanation:\n",
    "\n",
    "A custom block used throughout the architecture.\n",
    "\n",
    "- Composed of:\n",
    "\n",
    "- Conv2d (no bias → since batchnorm handles it),\n",
    "\n",
    "- BatchNorm2d for faster convergence,\n",
    "\n",
    "- LeakyReLU(0.1) as activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c4f39",
   "metadata": {},
   "source": [
    "#### 📌 **Why we create it:**\n",
    "\n",
    "* YOLOv1 uses many **repetitive conv blocks**, each with:\n",
    "\n",
    "  * A Convolutional Layer (to extract spatial features),\n",
    "  * Batch Normalization (to stabilize and speed up training),\n",
    "  * LeakyReLU activation (helps avoid dying ReLU problem).\n",
    "\n",
    "#### 🔧 **What it does:**\n",
    "\n",
    "* Applies feature transformation via learned filters (Conv2D),\n",
    "* Normalizes outputs (BatchNorm),\n",
    "* Adds non-linearity (LeakyReLU).\n",
    "\n",
    "#### 🟩 **Output:**\n",
    "\n",
    "* A 4D tensor: `[batch_size, out_channels, H, W]`\n",
    "* Represents extracted features from the input or previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f1fb3",
   "metadata": {},
   "source": [
    "### 🔷 2. YoloV1 Class – Main Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df12d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463caca",
   "metadata": {},
   "source": [
    "✅ Explanation:\n",
    "\n",
    "In __init__, the model:\n",
    "\n",
    "- Reads architecture_config (your earlier list).\n",
    "\n",
    "- Creates convolutional layers using _create_conv_layers().\n",
    "\n",
    "- Adds fully connected layers using _create_fcs()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10fb57",
   "metadata": {},
   "source": [
    "### 🔷 3. forward() Method – Forward Pass Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b37fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.darknet(x)\n",
    "    return self.fcs(torch.flatten(x, start_dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303002bf",
   "metadata": {},
   "source": [
    "#### 📌 **Why we create it:**\n",
    "\n",
    "* Defines how the input image flows through the model to get predictions.\n",
    "\n",
    "#### 🔧 **What it does:**\n",
    "\n",
    "* Passes input through:\n",
    "\n",
    "  * `darknet` → CNN backbone,\n",
    "  * `fcs` → Fully connected head after flattening the CNN output.\n",
    "\n",
    "#### 🟩 **Output:**\n",
    "\n",
    "* A **single flat prediction vector** for the entire image:\n",
    "\n",
    "  * Divided into S×S grid cells,\n",
    "  * For each cell: predictions for class and bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889d04b",
   "metadata": {},
   "source": [
    "### 🔷 4. _create_conv_layers() – CNN Backbone Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a04e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_conv_layers(self, architecture):\n",
    "    layers = []\n",
    "    in_channels = self.in_channels\n",
    "    \n",
    "    for x in architecture:\n",
    "        if type(x) == tuple:\n",
    "            layers += [CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3])]\n",
    "            in_channels = x[1]\n",
    "        \n",
    "        elif type(x) == str:\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        \n",
    "        elif type(x) == list:\n",
    "            conv1 = x[0]\n",
    "            conv2 = x[1]\n",
    "            repeats = x[2]\n",
    "            \n",
    "            for _ in range(repeats):\n",
    "                layers += [CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n",
    "                layers += [CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n",
    "                in_channels = conv2[1]\n",
    "    \n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498dbf82",
   "metadata": {},
   "source": [
    "#### 📌 **Why we create it:**\n",
    "\n",
    "* YOLOv1 uses a custom CNN (inspired by GoogLeNet) to extract **rich feature maps** from the input image.\n",
    "* These convolutional layers **encode position, texture, and shape** of objects.\n",
    "\n",
    "#### 🔧 **What it does:**\n",
    "\n",
    "* Converts the `architecture_config` into actual PyTorch layers:\n",
    "\n",
    "  * `Tuple` → Single CNNBlock.\n",
    "  * `\"M\"` → MaxPool (downsamples spatial resolution).\n",
    "  * `List` → Repeated mini CNN-blocks.\n",
    "\n",
    "#### 🟩 **Output:**\n",
    "\n",
    "* A tensor of shape `[batch_size, 1024, S, S]` (typically S = 7).\n",
    "* Each of the `S x S` grid cells contains **deep features** used to:\n",
    "\n",
    "  * Predict bounding boxes,\n",
    "  * Predict class probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae3bc6",
   "metadata": {},
   "source": [
    "### 🔷 5. _create_fcs() – Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d320a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "    S, B, C = split_size, num_boxes, num_classes\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(1024 * S * S, 496),  # Optionally change to 4096 for original paper\n",
    "        nn.Dropout(0.0),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Linear(496, S * S * (C + B * 5))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6cf12",
   "metadata": {},
   "source": [
    "### 🔷 3. `_create_fcs()` – Fully Connected Layers (Prediction Head)\n",
    "\n",
    "#### 📌 **Why we create it:**\n",
    "\n",
    "* After convolution, YOLO flattens the spatial feature map to feed into a prediction head.\n",
    "* The fully connected layers predict:\n",
    "\n",
    "  * Class probabilities,\n",
    "  * Bounding box coordinates,\n",
    "  * Confidence scores.\n",
    "\n",
    "#### 🔧 **What it does:**\n",
    "\n",
    "* `nn.Flatten()` → Flattens `[batch_size, 1024, 7, 7]` → `[batch_size, 1024*7*7]`\n",
    "* `nn.Linear(...)` → Reduces dimension to 496 (or 4096 in original paper),\n",
    "* `nn.Dropout` & `LeakyReLU` → Regularization & non-linearity,\n",
    "* Final `Linear` layer outputs `S * S * (C + B×5)`.\n",
    "\n",
    "#### 🟩 **Output:**\n",
    "\n",
    "* `[batch_size, S*S*(C + B*5)]` → Can be reshaped to `[batch_size, S, S, C + B*5]`\n",
    "* For each grid cell:\n",
    "\n",
    "  * C class scores,\n",
    "  * B bounding boxes (x, y, w, h, confidence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d042676",
   "metadata": {},
   "source": [
    "### Example Flow (assuming 448×448 input and S=7, B=2, C=20):\n",
    "\n",
    "| Stage       | Input Size         | Output Size       | Purpose                          |\n",
    "| ----------- | ------------------ | ----------------- | -------------------------------- |\n",
    "| Conv Layers | `[1, 3, 448, 448]` | `[1, 1024, 7, 7]` | Extract features                 |\n",
    "| Flatten     | `[1, 1024, 7, 7]`  | `[1, 50176]`      | Prepare for dense layers         |\n",
    "| FC Layer 1  | `[1, 50176]`       | `[1, 496]`        | Compress features                |\n",
    "| FC Layer 2  | `[1, 496]`         | `[1, 1470]`       | Final prediction (7×7×30)        |\n",
    "| Reshape     | `[1, 1470]`        | `[1, 7, 7, 30]`   | Class & bbox prediction per grid |\n",
    "\n",
    "\n",
    "\n",
    "### Breakdown of Final Output: `7x7x30`\n",
    "\n",
    "* 7×7 grid → 49 cells.\n",
    "* For each cell:\n",
    "\n",
    "  * 20 class scores (C),\n",
    "  * 2 bounding boxes (B=2), each with 5 values (x, y, w, h, confidence).\n",
    "\n",
    "Total: `7 × 7 × (20 + 2×5) = 7 × 7 × 30 = 1470`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebd339",
   "metadata": {},
   "source": [
    "### Complete Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea28a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "    \n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "    \n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3])]\n",
    "                in_channels = x[1]\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0] #Tuple\n",
    "                conv2 = x[1] #Tuple\n",
    "                repeats = x[2] #Int\n",
    "                \n",
    "                for _ in range(repeats):\n",
    "                    layers += [CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n",
    "                    layers += [CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n",
    "                    in_channels = conv2[1]\n",
    "                    \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(nn.Flatten(), nn.Linear(1024 * S * S, 496), nn.Dropout(0.0), nn.LeakyReLU(0.1), nn.Linear(496, S * S * (C + B * 5)))#Original paper uses nn.Linear(1024 * S * S, 4096) not 496. Also the last layer will be reshaped to (S, S, 13) where C+B*5 = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30e29f",
   "metadata": {},
   "source": [
    "## **Intersection Over Union**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5eb333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively.\n",
    "    \n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    #boxes_labels shape is (n, 4)\n",
    "    \n",
    "    if box_format == 'midpoint':\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "        \n",
    "    if box_format == 'corners':\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4] # Output tensor should be (N, 1). If we only use 3, we go to (N)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    \n",
    "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    \n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d725a",
   "metadata": {},
   "source": [
    "## 🔶 Purpose of the Function:\n",
    "\n",
    "To compute **IoU = Area of Overlap / Area of Union** between predicted and true bounding boxes.\n",
    "\n",
    "It supports two formats:\n",
    "\n",
    "* `\"midpoint\"`: boxes given as (x\\_center, y\\_center, width, height)\n",
    "* `\"corners\"`: boxes given as (x1, y1, x2, y2)\n",
    "\n",
    "\n",
    "\n",
    "## 🧱 Step-by-Step Explanation\n",
    "\n",
    "### ✅ Step 1: Convert box format (if needed)\n",
    "\n",
    "```python\n",
    "if box_format == 'midpoint':\n",
    "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "    ...\n",
    "```\n",
    "\n",
    "* This converts `(x_center, y_center, width, height)` → `(x1, y1, x2, y2)` for both predictions and labels.\n",
    "* Required because intersection calculation is easier in corner format.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Step 2: Extract corners directly (if already in \"corners\" format)\n",
    "\n",
    "```python\n",
    "if box_format == 'corners':\n",
    "    box1_x1 = boxes_preds[..., 0:1]\n",
    "    box1_y1 = boxes_preds[..., 1:2]\n",
    "    box1_x2 = boxes_preds[..., 2:3]\n",
    "    box1_y2 = boxes_preds[..., 3:4]\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Step 3: Calculate the intersection coordinates\n",
    "\n",
    "```python\n",
    "x1 = torch.max(box1_x1, box2_x1)\n",
    "y1 = torch.max(box1_y1, box2_y1)\n",
    "x2 = torch.min(box1_x2, box2_x2)\n",
    "y2 = torch.min(box1_y2, box2_y2)\n",
    "```\n",
    "\n",
    "* These are the coordinates of the **intersection rectangle**.\n",
    "* Only the overlapping region between the two boxes.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Step 4: Calculate the area of intersection\n",
    "\n",
    "```python\n",
    "intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "```\n",
    "\n",
    "* `(x2 - x1)` and `(y2 - y1)` give width and height of overlap.\n",
    "* `clamp(0)` ensures **no negative values** (if boxes don’t intersect).\n",
    "* Area = width × height.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Step 5: Calculate individual areas\n",
    "\n",
    "```python\n",
    "box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "```\n",
    "\n",
    "* Just standard area of a rectangle = (x2 - x1) × (y2 - y1)\n",
    "* `abs` ensures valid area even if coordinates are flipped.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Step 6: Compute IoU\n",
    "\n",
    "```python\n",
    "return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "```\n",
    "\n",
    "* **Union** = Area of box1 + Area of box2 - Area of intersection\n",
    "* Add `1e-6` to denominator to avoid division by zero\n",
    "* Final result = **IoU**, ranges from 0 to 1.\n",
    "\n",
    "\n",
    "\n",
    "## 🔁 Input/Output Shape Summary\n",
    "\n",
    "| Input          | Shape                            |\n",
    "| -------------- | -------------------------------- |\n",
    "| `boxes_preds`  | `[N, 4]`                         |\n",
    "| `boxes_labels` | `[N, 4]`                         |\n",
    "| Output         | `[N, 1]` (IoU for each box pair) |\n",
    "\n",
    "\n",
    "\n",
    "## 🧠 Use Case in YOLO\n",
    "\n",
    "This function is critical for:\n",
    "\n",
    "* Calculating **loss** during training (to determine how well boxes match),\n",
    "* Applying **Non-Max Suppression** (to eliminate duplicate detections).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f12d0",
   "metadata": {},
   "source": [
    "### **Non-Max Supression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c80ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e0fd7",
   "metadata": {},
   "source": [
    "### ✅ **Function Goal**\n",
    "\n",
    "Filter out overlapping bounding boxes that likely refer to the same object, keeping only the one with the highest confidence.\n",
    "\n",
    "\n",
    "\n",
    "### 📥 **Input Parameters**\n",
    "\n",
    "* `bboxes`: A list of predicted bounding boxes, where each box is of the form:\n",
    "  `[class_pred, prob_score, x1, y1, x2, y2]`\n",
    "\n",
    "* `iou_threshold`: If IoU between two boxes exceeds this, the box with the lower score is suppressed.\n",
    "\n",
    "* `threshold`: Minimum probability score for a box to be considered.\n",
    "\n",
    "* `box_format`: `\"corners\"` (x1, y1, x2, y2) or `\"midpoint\"` (cx, cy, w, h)\n",
    "\n",
    "\n",
    "\n",
    "### 🔢 **Step-by-Step Explanation**\n",
    "\n",
    "#### **1. Ensure input type is list**\n",
    "\n",
    "```python\n",
    "assert type(bboxes) == list\n",
    "```\n",
    "\n",
    "Safeguard: NMS expects a list of lists.\n",
    "\n",
    "\n",
    "\n",
    "#### **2. Remove low-confidence boxes**\n",
    "\n",
    "```python\n",
    "bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "```\n",
    "\n",
    "Keep only boxes with a confidence score higher than the threshold.\n",
    "\n",
    "\n",
    "\n",
    "#### **3. Sort by confidence score (descending)**\n",
    "\n",
    "```python\n",
    "bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "```\n",
    "\n",
    "This ensures that the highest confidence box is always selected first.\n",
    "\n",
    "\n",
    "\n",
    "#### **4. NMS Loop**\n",
    "\n",
    "```python\n",
    "while bboxes:\n",
    "    chosen_box = bboxes.pop(0)\n",
    "```\n",
    "\n",
    "Pick the highest-scoring box (`chosen_box`) and remove it from the list.\n",
    "\n",
    "\n",
    "\n",
    "#### **5. Filter Remaining Boxes**\n",
    "\n",
    "```python\n",
    "bboxes = [\n",
    "    box\n",
    "    for box in bboxes\n",
    "    if box[0] != chosen_box[0]\n",
    "    or intersection_over_union(\n",
    "        torch.tensor(chosen_box[2:]),\n",
    "        torch.tensor(box[2:]),\n",
    "        box_format=box_format,\n",
    "    ) < iou_threshold\n",
    "]\n",
    "```\n",
    "\n",
    "* Loop over the rest of the boxes.\n",
    "* Remove:\n",
    "\n",
    "  * Any box of the **same class**,\n",
    "  * With **IoU > iou\\_threshold** with the chosen box.\n",
    "* This suppresses duplicates of the same object.\n",
    "\n",
    "\n",
    "\n",
    "#### **6. Add Chosen Box to Final List**\n",
    "\n",
    "```python\n",
    "bboxes_after_nms.append(chosen_box)\n",
    "```\n",
    "\n",
    "Retain the box with highest confidence and acceptable overlap.\n",
    "\n",
    "\n",
    "\n",
    "#### **7. Return Final Filtered Boxes**\n",
    "\n",
    "```python\n",
    "return bboxes_after_nms\n",
    "```\n",
    "\n",
    "After the loop, this list contains only the most confident and non-overlapping boxes.\n",
    "\n",
    "\n",
    "\n",
    "### 🧠 Why Important in YOLO?\n",
    "\n",
    "YOLO predicts multiple boxes per grid cell. **Non-Max Suppression** ensures that only the best box per object is kept, preventing duplicated detections.\n",
    "\n",
    "\n",
    "\n",
    "### 📦 Example Input:\n",
    "\n",
    "```python\n",
    "[\n",
    " [0, 0.9, 10, 20, 30, 40],   # Class 0, high confidence\n",
    " [0, 0.85, 12, 22, 32, 42],  # Class 0, overlaps highly → likely removed\n",
    " [1, 0.8, 50, 50, 70, 70],   # Different class → retained\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a90d85",
   "metadata": {},
   "source": [
    "### **Mean Average Precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2253520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5602c8e",
   "metadata": {},
   "source": [
    "### 🎯 **Goal**\n",
    "\n",
    "To compute the **mean Average Precision (mAP)** across all classes.\n",
    "It measures how well your model predicts bounding boxes for all objects in all classes.\n",
    "\n",
    "\n",
    "\n",
    "### 📥 **Inputs**\n",
    "\n",
    "* `pred_boxes`: All predicted boxes in the format\n",
    "  `[image_idx, class_pred, prob_score, x1, y1, x2, y2]`\n",
    "* `true_boxes`: All ground-truth boxes in the same format.\n",
    "* `iou_threshold`: Minimum IoU needed to count a prediction as correct.\n",
    "* `box_format`: Either `\"midpoint\"` (cx, cy, w, h) or `\"corners\"` (x1, y1, x2, y2)\n",
    "* `num_classes`: Total number of object classes.\n",
    "\n",
    "\n",
    "\n",
    "## 🔁 Step-by-Step Breakdown\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Preparation**\n",
    "\n",
    "```python\n",
    "average_precisions = []\n",
    "epsilon = 1e-6\n",
    "```\n",
    "\n",
    "* `average_precisions`: To store AP per class\n",
    "* `epsilon`: Small number to avoid division by zero\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Loop through each class**\n",
    "\n",
    "```python\n",
    "for c in range(num_classes):\n",
    "    detections = []\n",
    "    ground_truths = []\n",
    "```\n",
    "\n",
    "We compute AP class-wise. For each class `c`, we gather:\n",
    "\n",
    "* All detections (predicted bboxes of class `c`)\n",
    "* All ground truths of class `c`\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Separate detections and GTs for this class**\n",
    "\n",
    "```python\n",
    "for detection in pred_boxes:\n",
    "    if detection[1] == c:\n",
    "        detections.append(detection)\n",
    "\n",
    "for true_box in true_boxes:\n",
    "    if true_box[1] == c:\n",
    "        ground_truths.append(true_box)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Count number of GT bboxes per image**\n",
    "\n",
    "```python\n",
    "amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "for key, val in amount_bboxes.items():\n",
    "    amount_bboxes[key] = torch.zeros(val)\n",
    "```\n",
    "\n",
    "* `amount_bboxes[img_id] = [0, 0, 0]` means there are 3 GT boxes in that image and none has been matched yet.\n",
    "\n",
    "\n",
    "\n",
    "### 5. **Sort detections by confidence**\n",
    "\n",
    "```python\n",
    "detections.sort(key=lambda x: x[2], reverse=True)\n",
    "TP = torch.zeros((len(detections)))\n",
    "FP = torch.zeros((len(detections)))\n",
    "total_true_bboxes = len(ground_truths)\n",
    "```\n",
    "\n",
    "* This helps us evaluate the most confident predictions first.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Skip if no ground truths**\n",
    "\n",
    "```python\n",
    "if total_true_bboxes == 0:\n",
    "    continue\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 7. **Evaluate each detection**\n",
    "\n",
    "```python\n",
    "for detection_idx, detection in enumerate(detections):\n",
    "    ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
    "```\n",
    "\n",
    "* For the predicted box, get all GT boxes from the same image.\n",
    "\n",
    "\n",
    "\n",
    "### 8. **Find best IoU match**\n",
    "\n",
    "```python\n",
    "best_iou = 0\n",
    "for idx, gt in enumerate(ground_truth_img):\n",
    "    iou = intersection_over_union(...)\n",
    "    if iou > best_iou:\n",
    "        best_iou = iou\n",
    "        best_gt_idx = idx\n",
    "```\n",
    "\n",
    "Compare detection to all GTs from same image, keep highest IoU.\n",
    "\n",
    "\n",
    "\n",
    "### 9. **Mark True Positive or False Positive**\n",
    "\n",
    "```python\n",
    "if best_iou > iou_threshold:\n",
    "    if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "        TP[detection_idx] = 1\n",
    "        amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "    else:\n",
    "        FP[detection_idx] = 1\n",
    "else:\n",
    "    FP[detection_idx] = 1\n",
    "```\n",
    "\n",
    "* If IoU is high and GT not already used → TP\n",
    "* If IoU is high but GT already matched → FP\n",
    "* If IoU is low → FP\n",
    "\n",
    "\n",
    "\n",
    "### 10. **Cumulative Precision and Recall**\n",
    "\n",
    "```python\n",
    "TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "```\n",
    "\n",
    "These vectors help us plot the **Precision-Recall (PR) curve**.\n",
    "\n",
    "\n",
    "\n",
    "### 11. **Add end points for smooth curve**\n",
    "\n",
    "```python\n",
    "precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 12. **Integrate Precision-Recall Curve**\n",
    "\n",
    "```python\n",
    "average_precisions.append(torch.trapz(precisions, recalls))\n",
    "```\n",
    "\n",
    "* `trapz()` performs numerical integration (area under PR curve = Average Precision)\n",
    "\n",
    "\n",
    "\n",
    "### 13. **Return mean of all APs**\n",
    "\n",
    "```python\n",
    "return sum(average_precisions) / len(average_precisions)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ **Final Output**\n",
    "\n",
    "Returns **mean Average Precision (mAP)** — a value between 0 and 1. Higher = better detection performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6091a031",
   "metadata": {},
   "source": [
    "### Get Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda74f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    pred_format=\"cells\",\n",
    "    box_format=\"midpoint\",\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "\n",
    "            #if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "            #    print(nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73528d",
   "metadata": {},
   "source": [
    "### 🎯 **Goal**\n",
    "\n",
    "To collect all predicted and ground truth bounding boxes from a dataset loader for computing metrics like mAP.\n",
    "\n",
    "\n",
    "\n",
    "## 📥 Parameters\n",
    "\n",
    "```python\n",
    "loader         # Dataloader with (images, labels)\n",
    "model          # YOLO model\n",
    "iou_threshold  # IoU for NMS and AP calculation\n",
    "threshold      # Confidence threshold for predictions\n",
    "pred_format    # Format like \"cells\" (YOLO grid output)\n",
    "box_format     # \"midpoint\" or \"corners\"\n",
    "device         # \"cuda\" or \"cpu\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🔁 Breakdown\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Initialize**\n",
    "\n",
    "```python\n",
    "all_pred_boxes = []\n",
    "all_true_boxes = []\n",
    "model.eval()\n",
    "train_idx = 0\n",
    "```\n",
    "\n",
    "* `all_pred_boxes`: all predicted boxes, per image\n",
    "* `all_true_boxes`: all ground-truth boxes, per image\n",
    "* `train_idx`: tracks image index across batches (used for mAP calculation)\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Loop Through Batches**\n",
    "\n",
    "```python\n",
    "for batch_idx, (x, labels) in enumerate(loader):\n",
    "```\n",
    "\n",
    "* `x`: images\n",
    "* `labels`: ground-truth boxes (YOLO format)\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Move to device and predict**\n",
    "\n",
    "```python\n",
    "x = x.to(device)\n",
    "labels = labels.to(device)\n",
    "with torch.no_grad():\n",
    "    predictions = model(x)\n",
    "```\n",
    "\n",
    "* Perform forward pass without tracking gradients.\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Convert Cell Format to Bounding Boxes**\n",
    "\n",
    "```python\n",
    "true_bboxes = cellboxes_to_boxes(labels)\n",
    "bboxes = cellboxes_to_boxes(predictions)\n",
    "```\n",
    "\n",
    "* Converts YOLO’s grid/cell outputs into `[class, prob, x1, y1, x2, y2]` box format for evaluation.\n",
    "\n",
    "\n",
    "\n",
    "### 5. **Loop through individual images in batch**\n",
    "\n",
    "```python\n",
    "for idx in range(batch_size):\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 6. **Apply NMS to predicted boxes**\n",
    "\n",
    "```python\n",
    "nms_boxes = non_max_suppression(\n",
    "    bboxes[idx],\n",
    "    iou_threshold=iou_threshold,\n",
    "    threshold=threshold,\n",
    "    box_format=box_format,\n",
    ")\n",
    "```\n",
    "\n",
    "* Removes overlapping predicted boxes using IoU threshold.\n",
    "\n",
    "\n",
    "\n",
    "### 7. **Store predictions**\n",
    "\n",
    "```python\n",
    "for nms_box in nms_boxes:\n",
    "    all_pred_boxes.append([train_idx] + nms_box)\n",
    "```\n",
    "\n",
    "* Add predictions for this image to `all_pred_boxes`\n",
    "  (with `train_idx` as image identifier)\n",
    "\n",
    "\n",
    "\n",
    "### 8. **Store ground truth boxes**\n",
    "\n",
    "```python\n",
    "for box in true_bboxes[idx]:\n",
    "    if box[1] > threshold:  # filter low-confidence GT boxes\n",
    "        all_true_boxes.append([train_idx] + box)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 9. **Increment image index**\n",
    "\n",
    "```python\n",
    "train_idx += 1\n",
    "```\n",
    "\n",
    "Each image gets a unique index for comparison during mAP calculation.\n",
    "\n",
    "\n",
    "\n",
    "### 10. **Return model to training mode**\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "return all_pred_boxes, all_true_boxes\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Output\n",
    "\n",
    "* `all_pred_boxes`: List of `[image_id, class, prob, x1, y1, x2, y2]`\n",
    "* `all_true_boxes`: Same format, without NMS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600aaebd",
   "metadata": {},
   "source": [
    "### `convert_cellboxes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690a22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=7, C=3):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios. Tried to do this\n",
    "    vectorized, but this resulted in quite difficult to read\n",
    "    code... Use as a black box? Or implement a more intuitive,\n",
    "    using 2 for loops iterating range(S) and convert them one\n",
    "    by one, resulting in a slower but more readable implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n",
    "    bboxes1 = predictions[..., C + 1:C + 5]\n",
    "    bboxes2 = predictions[..., C + 6:C + 10]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76e474",
   "metadata": {},
   "source": [
    "### 🎯 **Goal**\n",
    "\n",
    "Convert YOLO outputs (from grid cells) into bounding boxes normalized w\\.r.t. the **whole image**, in `[class, confidence, x, y, w, h]` format.\n",
    "\n",
    "\n",
    "\n",
    "## 📥 Parameters\n",
    "\n",
    "```python\n",
    "predictions: Tensor of shape (N, 7, 7, C + 10)\n",
    "    # Where:\n",
    "    #   C = number of classes\n",
    "    #   10 = 2 boxes per cell (each with [x, y, w, h, confidence])\n",
    "S: Grid size (usually 7)\n",
    "C: Number of classes\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🔁 Step-by-Step Explanation\n",
    "\n",
    "### 1. **Reshape**\n",
    "\n",
    "```python\n",
    "predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n",
    "```\n",
    "\n",
    "From flat vector → structured grid: 7×7 cells each with 2 boxes and `C` class scores.\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Extract Bounding Boxes**\n",
    "\n",
    "```python\n",
    "bboxes1 = predictions[..., C + 1:C + 5]  # [x1, y1, w1, h1]\n",
    "bboxes2 = predictions[..., C + 6:C + 10] # [x2, y2, w2, h2]\n",
    "```\n",
    "\n",
    "Two predicted boxes per grid cell.\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Select Best Box (Highest Confidence)**\n",
    "\n",
    "```python\n",
    "scores = torch.cat((predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0)\n",
    "best_box = scores.argmax(0).unsqueeze(-1)\n",
    "best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "```\n",
    "\n",
    "* Get best box across the 2 using objectness scores.\n",
    "* Use soft masking to pick either bboxes1 or bboxes2 per cell.\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Convert Cell-Relative to Image-Relative Coordinates**\n",
    "\n",
    "```python\n",
    "cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "w_y = 1 / S * best_boxes[..., 2:4]\n",
    "```\n",
    "\n",
    "* Convert from **cell-relative** (e.g., x in \\[0,1]) to **image-relative** (i.e., x in \\[0,1] across whole image).\n",
    "* Add cell offset (`i`, `j`) then scale by `1/S`.\n",
    "\n",
    "\n",
    "\n",
    "### 5. **Concatenate Final Bounding Box**\n",
    "\n",
    "```python\n",
    "converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 6. **Get Class & Confidence**\n",
    "\n",
    "```python\n",
    "predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(-1)\n",
    "```\n",
    "\n",
    "* Highest class probability.\n",
    "* Highest confidence of the two boxes.\n",
    "\n",
    "\n",
    "\n",
    "### 7. **Final Output**\n",
    "\n",
    "```python\n",
    "converted_preds = torch.cat((predicted_class, best_confidence, converted_bboxes), dim=-1)\n",
    "return converted_preds\n",
    "```\n",
    "\n",
    "Shape: `(N, 7, 7, 6)` with values:\n",
    "\n",
    "```\n",
    "[class, confidence, x, y, w, h]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092ddd3",
   "metadata": {},
   "source": [
    "### `cellboxes_to_boxes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20c9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c9f36",
   "metadata": {},
   "source": [
    "### ✅ `cellboxes_to_boxes(out, S=7)`\n",
    "\n",
    "**Purpose**:\n",
    "Converts YOLO output tensor from model into a list of bounding boxes per image, using the format:\n",
    "\n",
    "```\n",
    "[class, confidence, x, y, w, h]\n",
    "```\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. Calls `convert_cellboxes()` to convert YOLO cell predictions into image-relative boxes.\n",
    "2. Reshapes it into `(batch_size, 49, 6)` — because there are 49 (7×7) cells per image.\n",
    "3. Converts class index to integer.\n",
    "4. Loops through each image and stores the list of boxes.\n",
    "\n",
    "**Returns**:\n",
    "List of bounding boxes for each image in the batch:\n",
    "\n",
    "```python\n",
    "[\n",
    "  [[class, conf, x, y, w, h], [...], ...],  # Image 1\n",
    "  [[class, conf, x, y, w, h], [...], ...],  # Image 2\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 💾 `save_checkpoint(state, filename=\"my_checkpoint.pth\")`\n",
    "\n",
    "**Purpose**:\n",
    "Saves the model state and optimizer state to a file.\n",
    "\n",
    "**Typical Usage**:\n",
    "\n",
    "```python\n",
    "save_checkpoint({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "})\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 📦 `load_checkpoint(checkpoint, model, optimizer)`\n",
    "\n",
    "**Purpose**:\n",
    "Loads a previously saved checkpoint into the model and optimizer.\n",
    "\n",
    "**Typical Usage**:\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(\"my_checkpoint.pth\")\n",
    "load_checkpoint(checkpoint, model, optimizer)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2425e",
   "metadata": {},
   "source": [
    "## **Dataset Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = 'Data/train_zip/train'\n",
    "test_dir = 'Data/test_zip/test'\n",
    "\n",
    "images = [image for image in sorted(os.listdir(files_dir))\n",
    "                        if image[-4:]=='.jpg']\n",
    "annots = []\n",
    "for image in images:\n",
    "    annot = image[:-4] + '.xml'\n",
    "    annots.append(annot)\n",
    "    \n",
    "images = pd.Series(images, name='images')\n",
    "annots = pd.Series(annots, name='annots')\n",
    "df = pd.concat([images, annots], axis=1)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "test_images = [image for image in sorted(os.listdir(test_dir))\n",
    "                        if image[-4:]=='.jpg']\n",
    "\n",
    "test_annots = []\n",
    "for image in test_images:\n",
    "    annot = image[:-4] + '.xml'\n",
    "    test_annots.append(annot)\n",
    "\n",
    "test_images = pd.Series(test_images, name='test_images')\n",
    "test_annots = pd.Series(test_annots, name='test_annots')\n",
    "test_df = pd.concat([test_images, test_annots], axis=1)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d1e852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>annots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple_1.jpg</td>\n",
       "      <td>apple_1.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple_10.jpg</td>\n",
       "      <td>apple_10.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple_11.jpg</td>\n",
       "      <td>apple_11.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple_12.jpg</td>\n",
       "      <td>apple_12.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple_13.jpg</td>\n",
       "      <td>apple_13.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>apple_14.jpg</td>\n",
       "      <td>apple_14.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>apple_15.jpg</td>\n",
       "      <td>apple_15.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>apple_16.jpg</td>\n",
       "      <td>apple_16.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>apple_17.jpg</td>\n",
       "      <td>apple_17.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apple_18.jpg</td>\n",
       "      <td>apple_18.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>apple_19.jpg</td>\n",
       "      <td>apple_19.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          images        annots\n",
       "0    apple_1.jpg   apple_1.xml\n",
       "1   apple_10.jpg  apple_10.xml\n",
       "2   apple_11.jpg  apple_11.xml\n",
       "3   apple_12.jpg  apple_12.xml\n",
       "4   apple_13.jpg  apple_13.xml\n",
       "5   apple_14.jpg  apple_14.xml\n",
       "6   apple_15.jpg  apple_15.xml\n",
       "7   apple_16.jpg  apple_16.xml\n",
       "8   apple_17.jpg  apple_17.xml\n",
       "9   apple_18.jpg  apple_18.xml\n",
       "10  apple_19.jpg  apple_19.xml"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(image_dir, image_col, annot_col):\n",
    "    images = sorted([img for img in os.listdir(image_dir) if img.endswith('.jpg')])\n",
    "    annots = [img.replace('.jpg', '.xml') for img in images]\n",
    "    return pd.DataFrame({image_col: images, annot_col: annots})\n",
    "\n",
    "# Paths\n",
    "train_dir = 'Data/train_zip/train'\n",
    "test_dir = 'Data/test_zip/test'\n",
    "\n",
    "# Create DataFrames\n",
    "df = create_dataframe(train_dir, 'images', 'annots')\n",
    "test_df = create_dataframe(test_dir, 'test_images', 'test_annots')\n",
    "\n",
    "df.head(11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fdc2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitImagesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=df, files_dir=train_dir, S=7, B=2, C=3, transform=None):\n",
    "        self.annotations = df\n",
    "        self.files_dir = files_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.files_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        tree = ET.parse(label_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        class_dictionary = {'apple':0, 'banana':1, 'orange':2}\n",
    "    \n",
    "        if(int(root.find('size').find('height').text) == 0):\n",
    "            filename = root.find('filename').text\n",
    "            img = Image.open(self.files_dir + '/' + filename)\n",
    "            img_width, img_height = img.size\n",
    "            \n",
    "            for member in root.findall('object'):\n",
    "            \n",
    "                klass = member.find('name').text\n",
    "                klass = class_dictionary[klass]\n",
    "            \n",
    "                # bounding box\n",
    "                xmin = int(member.find('bndbox').find('xmin').text)\n",
    "                xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            \n",
    "                ymin = int(member.find('bndbox').find('ymin').text)\n",
    "                ymax = int(member.find('bndbox').find('ymax').text)\n",
    "                \n",
    "                centerx = ((xmax + xmin) / 2) / img_width\n",
    "                centery = ((ymax + ymin) / 2) / img_height\n",
    "                boxwidth = (xmax - xmin) / img_width\n",
    "                boxheight = (ymax - ymin) / img_height\n",
    "            \n",
    "            \n",
    "                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n",
    "            \n",
    "        elif(int(root.find('size').find('height').text) != 0):\n",
    "            \n",
    "            for member in root.findall('object'):\n",
    "            \n",
    "                klass = member.find('name').text\n",
    "                klass = class_dictionary[klass]\n",
    "            \n",
    "                                # bounding box\n",
    "                xmin = int(member.find('bndbox').find('xmin').text)\n",
    "                xmax = int(member.find('bndbox').find('xmax').text)\n",
    "                img_width = int(root.find('size').find('width').text)\n",
    "            \n",
    "                ymin = int(member.find('bndbox').find('ymin').text)\n",
    "                ymax = int(member.find('bndbox').find('ymax').text)\n",
    "                img_height = int(root.find('size').find('height').text)\n",
    "                \n",
    "                centerx = ((xmax + xmin) / 2) / img_width\n",
    "                centery = ((ymax + ymin) / 2) / img_height\n",
    "                boxwidth = (xmax - xmin) / img_width\n",
    "                boxheight = (ymax - ymin) / img_height\n",
    "            \n",
    "            \n",
    "                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n",
    "\n",
    "                \n",
    "        boxes = torch.tensor(boxes)\n",
    "        img_path = os.path.join(self.files_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image)\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            # i,j represents the cell row and cell column\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            \"\"\"\n",
    "            Calculating the width and height of cell of bounding box,\n",
    "            relative to the cell is done by the following, with\n",
    "            width as the example:\n",
    "            \n",
    "            width_pixels = (width*self.image_width)\n",
    "            cell_pixels = (self.image_width)\n",
    "            \n",
    "            Then to find the width relative to the cell is simply:\n",
    "            width_pixels/cell_pixels, simplification leads to the\n",
    "            formulas below.\n",
    "            \"\"\"\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            # If no object already found for specific cell i,j\n",
    "            # Note: This means we restrict to ONE object\n",
    "            # per cell!\n",
    "#             print(i, j)\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                # Set that there exists an object\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                # Box coordinates\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, 4:8] = box_coordinates\n",
    "\n",
    "                # Set one hot encoding for class_label\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440b777",
   "metadata": {},
   "source": [
    "The `FruitImagesDataset` class is designed to load fruit images and their associated bounding box annotations from XML files. It processes the images, converts bounding boxes into cell-based coordinates, and returns both the image and its corresponding label matrix.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Initialization (`__init__`)**:\n",
    "\n",
    "   * Takes a dataframe (`df`), directory paths (`files_dir`), and other configuration parameters like grid size (`S`), number of bounding boxes (`B`), number of classes (`C`), and optional transformations (`transform`).\n",
    "\n",
    "2. **Length Method (`__len__`)**:\n",
    "\n",
    "   * Returns the total number of items (images) in the dataset.\n",
    "\n",
    "3. **Get Item Method (`__getitem__`)**:\n",
    "\n",
    "   * Loads an image and its corresponding XML annotation file.\n",
    "   * Extracts bounding box details (class, center coordinates, width, and height).\n",
    "   * Converts coordinates to relative values with respect to the image size.\n",
    "   * Converts bounding box coordinates to a grid-based representation (cell coordinates in the `S x S` grid).\n",
    "\n",
    "4. **Label Matrix**:\n",
    "\n",
    "   * The label matrix is a `S x S x (C + 5 * B)` tensor, where:\n",
    "\n",
    "     * `C` represents the number of classes (fruit types).\n",
    "     * `5 * B` accounts for the bounding box parameters (center x, center y, width, height, and confidence score) for each bounding box.\n",
    "   * Each grid cell contains the bounding box information if a box is centered in that cell.\n",
    "\n",
    "5. **Transformation**:\n",
    "\n",
    "   * If a `transform` function is provided, it's applied to both the image and bounding box coordinates.\n",
    "\n",
    "### Suggestions for Improvement:\n",
    "\n",
    "1. **Transform Method**:\n",
    "\n",
    "   * You commented out `image = self.transform(image)`. If you're using data augmentation (like resizing, normalizing, or random flipping), you could apply transformations explicitly here.\n",
    "\n",
    "2. **Bounding Box Handling**:\n",
    "\n",
    "   * Your code assumes only one object per grid cell (`if label_matrix[i, j, self.C] == 0`). If you want to support multiple objects per cell, you'll need to extend this logic.\n",
    "\n",
    "3. **Handling Images with No Bounding Boxes**:\n",
    "\n",
    "   * For images that have no bounding boxes, you should ensure the `label_matrix` has only zeros to indicate no objects.\n",
    "\n",
    "4. **Class Dictionary**:\n",
    "\n",
    "   * The `class_dictionary` can be moved outside the class initialization if it's constant and doesn't need to be redefined for each instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e43f3b",
   "metadata": {},
   "source": [
    "### **Model Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3d62419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=3):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., self.C + 6:self.C + 10]\n",
    "                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32502e",
   "metadata": {},
   "source": [
    "The `YoloLoss` class calculates the loss for the YOLO (You Only Look Once) model, specifically for version 1. It implements several components of the YOLO loss function, including box loss, object loss, no object loss, and class loss, each with associated scaling factors.\n",
    "\n",
    "### Key Features of the `YoloLoss` Class:\n",
    "\n",
    "1. **Initialization (`__init__`)**:\n",
    "\n",
    "   * Initializes the loss function with parameters for grid size (`S`), number of bounding boxes (`B`), number of classes (`C`), and weights for no object loss (`lambda_noobj`) and box coordinate loss (`lambda_coord`).\n",
    "   * Uses `MSELoss` for computing the loss.\n",
    "\n",
    "2. **Forward Method (`forward`)**:\n",
    "\n",
    "   * Takes the `predictions` (model output) and `target` (ground truth) as inputs.\n",
    "   * Reshapes the predictions to match the grid size and class + box information.\n",
    "\n",
    "3. **IoU Calculation**:\n",
    "\n",
    "   * Calculates the intersection over union (IoU) between the predicted bounding boxes and the target bounding boxes for both the predicted boxes (`iou_b1` and `iou_b2`).\n",
    "   * Selects the bounding box with the highest IoU as the best prediction.\n",
    "\n",
    "4. **Box Coordinates Loss**:\n",
    "\n",
    "   * Computes the loss for box coordinates using the mean squared error (MSE) between predicted and target bounding boxes.\n",
    "   * The width and height are adjusted using the square root of the values to scale them properly.\n",
    "\n",
    "5. **Object Loss**:\n",
    "\n",
    "   * Calculates the object confidence loss based on the best bounding box for each grid cell (i.e., whether there is an object present).\n",
    "\n",
    "6. **No Object Loss**:\n",
    "\n",
    "   * Computes the loss for cells that don't contain any objects. The loss is scaled using the `lambda_noobj` factor.\n",
    "\n",
    "7. **Class Loss**:\n",
    "\n",
    "   * Computes the classification loss, comparing the predicted class probabilities to the target class labels for each grid cell.\n",
    "\n",
    "8. **Total Loss**:\n",
    "\n",
    "   * The final loss is a weighted sum of the box loss, object loss, no object loss, and class loss, with appropriate scaling factors (`lambda_coord`, `lambda_noobj`).\n",
    "\n",
    "### Breakdown of Loss Terms:\n",
    "\n",
    "* **Box Loss** (`lambda_coord * box_loss`): Penalizes incorrect bounding box coordinates.\n",
    "* **Object Loss** (`object_loss`): Penalizes cells that incorrectly predict the presence or absence of an object.\n",
    "* **No Object Loss** (`lambda_noobj * no_object_loss`): Penalizes cells that incorrectly predict no object.\n",
    "* **Class Loss** (`class_loss`): Penalizes incorrect class predictions.\n",
    "\n",
    "### Suggestions for Improvement:\n",
    "\n",
    "1. **IoU Calculation**:\n",
    "\n",
    "   * The `intersection_over_union` function should be implemented or imported. Ensure that it handles both cases correctly (when the bounding boxes overlap and when they do not).\n",
    "\n",
    "2. **Optimization**:\n",
    "\n",
    "   * The loss function uses element-wise MSE. You may want to consider if this is the best loss for bounding box predictions, as other loss functions like `SmoothL1Loss` or `GIoU` (Generalized IoU) can sometimes give better results in object detection tasks.\n",
    "\n",
    "3. **No Object Loss**:\n",
    "\n",
    "   * The commented-out `max_no_obj` portion could be useful if you want to calculate the loss based on the maximum of the \"no object\" confidence for each bounding box.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bf32a",
   "metadata": {},
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f6299e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 20\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f8aad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af0dc8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29eb2c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad10raza/anaconda3/envs/aiml-test/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 4/15 [00:03<00:08,  1.24it/s, loss=684]    /home/ahmad10raza/anaconda3/envs/aiml-test/lib/python3.9/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:11<00:00,  1.29it/s, loss=499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 624.2450846354167\n",
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.37it/s, loss=197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 277.68736368815104\n",
      "Train mAP: 0.000975356379058212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.46it/s, loss=175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 180.51609598795574\n",
      "Train mAP: 0.017863621935248375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.47it/s, loss=118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 136.6421910603841\n",
      "Train mAP: 0.12064716219902039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.43it/s, loss=101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 107.87092437744141\n",
      "Train mAP: 0.15986643731594086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:11<00:00,  1.36it/s, loss=71.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 92.60657552083333\n",
      "Train mAP: 0.24699532985687256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.47it/s, loss=64.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 80.96835149129232\n",
      "Train mAP: 0.34965309500694275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.47it/s, loss=80.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 74.83770319620768\n",
      "Train mAP: 0.41942086815834045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.46it/s, loss=55]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 67.00601298014323\n",
      "Train mAP: 0.559205174446106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.46it/s, loss=50.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 63.34628168741862\n",
      "Train mAP: 0.6099393367767334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.48it/s, loss=59]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 59.24650319417318\n",
      "Train mAP: 0.6723823547363281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.47it/s, loss=60]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 57.036339569091794\n",
      "Train mAP: 0.693082869052887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.49it/s, loss=80.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 53.85564333597819\n",
      "Train mAP: 0.7688582539558411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.48it/s, loss=57.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 50.629859415690106\n",
      "Train mAP: 0.7860605120658875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.48it/s, loss=41.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 46.72044245402018\n",
      "Train mAP: 0.8171432614326477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.46it/s, loss=49.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 48.8563850402832\n",
      "Train mAP: 0.8453867435455322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.48it/s, loss=43.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 50.4185910542806\n",
      "Train mAP: 0.8499798774719238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.46it/s, loss=32.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 40.82767804463705\n",
      "Train mAP: 0.8640470504760742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.47it/s, loss=46.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 43.815459696451825\n",
      "Train mAP: 0.8637433052062988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.47it/s, loss=35.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 40.661138916015624\n",
      "Train mAP: 0.883161723613739\n",
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "    train_dataset = FruitImagesDataset(\n",
    "        transform=transform,\n",
    "        files_dir=train_dir\n",
    "    )\n",
    "\n",
    "    test_dataset = FruitImagesDataset(\n",
    "        transform=transform, \n",
    "        files_dir=test_dir\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "        \n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "        \n",
    "        scheduler.step(mean_avg_prec)\n",
    "    \n",
    "    checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc506f69",
   "metadata": {},
   "source": [
    "### **Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb110833",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = True\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "048ef083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_419497/894028648.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.42it/s, loss=103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 168.57548713684082\n",
      "Test mAP: 0.19635097682476044\n"
     ]
    }
   ],
   "source": [
    "def predictions():\n",
    "    model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "    test_dataset = FruitImagesDataset(\n",
    "        transform=transform, \n",
    "        df=test_df,\n",
    "        files_dir=test_dir\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.eval()\n",
    "        train_fn(test_loader, model, optimizer, loss_fn)\n",
    "        \n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "        print(f\"Test mAP: {mean_avg_prec}\")\n",
    "\n",
    "\n",
    "predictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
